{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a74267-c536-4694-aa4f-6094c1392758",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Install kafka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499e39a3-6656-445e-abd6-859ebf2d1293",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "sudo wget https://downloads.apache.org/kafka/3.4.1/kafka_2.12-3.5.1.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73eeab39-407f-4fe4-b7d2-2adbb1b53882",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Unpacking Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07123b8b-e35b-4c04-8a85-e6c9feb52b61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "tar -xvf kafka_2.12-3.5.1.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d627fbd-f87e-4e8d-bf85-d090407d0e33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-04 21:03:00,564] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2023-10-04 21:03:02,618] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2023-10-04 21:03:03,073] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2023-10-04 21:03:03,083] INFO starting (kafka.server.KafkaServer)\n[2023-10-04 21:03:03,085] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n[2023-10-04 21:03:03,220] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 21:03:03,308] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,309] INFO Client environment:host.name=1004-194529-xqyu0ihu-10-172-209-186 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,309] INFO Client environment:java.version=1.8.0_372 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,310] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,311] INFO Client environment:java.home=/usr/lib/jvm/zulu8-ca-amd64/jre (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,315] INFO Client environment:java.class.path=/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/----ws_3_5--mvn--hadoop3--hadoop-azure-abfs--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:/databricks/jars/third_party--armeria--armeria_shaded--1805148092--io.netty__netty-resolver__4.1.86.Final.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__2.3.9.jar:/databricks/jars/common--logging--log4j--log4j-reset-spark_3.5_2.12_deploy.jar:/databricks/jars/common--rpc--util--util-spark_3.5_2.12_deploy.jar:/databricks/jars/common--instrumentation--error-state-recorder--error-state-recorder-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--jsonwebtoken--jsonwebtoken_shaded--922124789--com.fasterxml.jackson.core__jackson-databind__2.14.2.jar:/databricks/jars/----ws_3_5--vendor--file-notification-common--libfile-notification-common_resources.jar:/databricks/jars/proto--logs--activity--config-analyzer--config_analyzer_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded--414797747--io.grpc__grpc-context__1.45.1.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1093464845--io.netty__netty-codec-http2__4.1.86.Final.jar:/databricks/jars/third_party--opencensus-shaded--libopencensus_shaded.jar:/databricks/jars/proto--logs--app-state--app_state_collector_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1340455310--com.google.inject__guice__3.0.jar:/databricks/jars/----ws_3_5--vendor--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2--1705513748--software.amazon.awssdk__metrics-spi__2.20.121.jar:/databricks/jars/----ws_3_5--sql--catalyst--catalyst-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--xgboost-predictor--xgboost-predictor-shaded--503511358--net.jafama__jafama__2.1.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--2135166157--com.fasterxml.jackson.core__jackson-core__2.12.7.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.fasterxml.jackson.dataformat--jackson-dataformat-cbor--com.fasterxml.jackson.dataformat__jackson-dataformat-cbor__2.15.2.jar:/databricks/jars/common--logging--topic--topic-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12---1275080308--com.google.code.gson__gson__2.8.9.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-shaded-spark_3.5_2.12_deploy.jar:/databricks/jars/common--rpc--conf--conf-spark_3.5_2.12_deploy.jar:/databricks/jars/common--logging--audit--auditable-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1980141100--io.grpc__grpc-api__1.45.1.jar:/databricks/jars/common--activity-context--activity-context-spark_3.5_2.12_deploy.jar:/databricks/jars/common--database--toberefactored--toberefactored-spark_3.5_2.12_deploy.jar:/databricks/jars/common--util--timestamp-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1307550018--org.apache.httpcomponents__httpclient__4.5.13.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1312913098--com.google.j2objc__j2objc-annotations__1.3.jar:/databricks/jars/s3--s3-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--jetty-websocket-server--jetty-websocket-server--1182574203--org.eclipse.jetty.websocket__websocket-servlet__9.3.30.databricks.jar:/databricks/jars/common--encryption--cpk-deps-shaded--2084285423--io.netty__netty-resolver-dns__4.1.65.Final.jar:/databricks/jars/common--spark--version--version-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded---578504376--com.google.j2objc__j2objc-annotations__1.3.jar:/databricks/jars/common--jetty--shared--proto_converter-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-buffer--io.netty__netty-buffer__4.1.93.Final.jar:/databricks/jars/sqlgateway--history--api--client-spark_3.5_2.12_deploy.jar:/databricks/jars/common--rpc--metrics--metrics-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mllib-local--mllib-local-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.glassfish.jersey.containers--jersey-container-servlet-core--org.glassfish.jersey.containers__jersey-container-servlet-core__2.40.jar:/databricks/jars/proto--logs--activity--accounts-manager--accounts_manager_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12---1533069823--io.netty__netty-resolver-dns-native-macos__4.1.94.Final-osx-aarch_64.jar:/databricks/jars/common--encryption--proto--encryption_proto-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--531892547--org.checkerframework__checker-qual__3.12.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-config--com.amazonaws__aws-java-sdk-config__1.12.390.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.eclipse.jetty--jetty-io--org.eclipse.jetty__jetty-io__9.4.52.v20230823.jar:/databricks/jars/common--util--log_utils_proto_scala-spark_3.5_2.12_deploy.jar:/databricks/jars/feature-flag--common--model--version_value_lib-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--716333144--io.netty__netty-transport-classes-kqueue__4.1.94.Final.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.acplt.remotetea--remotetea-oncrpc--org.acplt.remotetea__remotetea-oncrpc__1.1.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.2.19.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded--14135965--com.google.api.grpc__proto-google-common-protos__2.0.1.jar:/databricks/jars/third_party--gcs-private--libmeta-services.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---334114139--com.azure__azure-core-http-netty__1.12.3.jar:/databricks/jars/----ws_3_5--vendor--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2---1760240397--software.amazon.awssdk__third-party-jackson-core__2.20.121.jar:/databricks/jars/common--command-result--command-result-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---940791386--org.apache.httpcomponents__httpclient__4.5.13.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---2136230945--io.netty__netty-tcnative-classes__2.0.54.Final.jar:/databricks/jars/----ws_3_5--vendor--kafka-0-10-hive-2.3__hadoop-3.2_2.12_shaded--480357566--kafka-0-10-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded--1596312874--io.grpc__grpc-stub__1.45.1.jar:/databricks/jars/spark--pipelines--api--api-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mllib--mllib-hive-2.3__hadoop-3.2_2.12_shaded---1793724848--mllib-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---932352778--io.grpc__grpc-auth__1.34.1.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---1225171404--org.codehaus.woodstox__stax2-api__4.2.1.jar:/databricks/jars/----ws_3_5--vendor--iceberg-core--api--immutables_shaded-for-iceberg-api-shaded-hive-2.3__hadoop-3.2---1798240091--libimmutables.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--net.java.dev.jna--jna--net.java.dev.jna__jna__5.8.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.scala-lang.modules--scala-collection-compat_2.12--org.scala-lang.modules__scala-collection-compat_2.12__2.9.0.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded--2127711412--io.grpc__grpc-protobuf-lite__1.45.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---932299027--io.grpc__grpc-core__1.34.1.jar:/databricks/jars/third_party--jetty-websocket-server--jetty-websocket-server--1525924053--org.eclipse.jetty.websocket__websocket-client__9.3.30.databricks.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---744856947--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/common--logging--structured--tag-util-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--290392895--com.lihaoyi__fastparse_2.12__2.1.3.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--httpcore-4.4.16.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--632524745--com.google.crypto.tink__tink__1.9.0.jar:/databricks/jars/common--driver-nfs--driver-nfs-spark_3.5_2.12_deploy.jar:/databricks/jars/----com_google_googleapis--google--rpc--code_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.fasterxml.jackson.module--jackson-module-scala_2.12--com.fasterxml.jackson.module__jackson-module-scala_2.12__2.15.2.jar:/databricks/jars/----ws_3_5--vendor--protobuf--kafka-clients_only_shaded-for-protobuf-hive-2.3__hadoop-3.2--224550366--org.apache.kafka__kafka-clients__7.4.0-ccs.jar:/databricks/jars/common--util--zip-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---2128655557--org.apache.commons__commons-lang3__3.5.jar:/databricks/jars/----ws_3_5--vendor--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--103274271--io.netty__netty-codec-http__4.1.86.Final.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1732777613--io.netty__netty-tcnative-boringssl-static__2.0.54.Final-osx-x86_64.jar:/databricks/jars/common--converters--converters-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12---536658286--io.netty__netty-transport-classes-epoll__4.1.94.Final.jar:/databricks/jars/----ws_3_5--vendor--redshift--redshift-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1843136381--com.google.apis__google-api-services-networkmanagement__v1-rev20200505-1.30.10.jar:/databricks/jars/common--credentials--credentials-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---1941234734--io.netty__netty-resolver__4.1.93.Final.jar:/databricks/jars/third_party--armeria--armeria_shaded---765956118--com.aayushatharva.brotli4j__brotli4j__1.7.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---366356933--commons-codec__commons-codec__1.11.jar:/databricks/jars/common--cluster--id--id-spark_3.5_2.12_deploy.jar:/databricks/jars/common--jobs--task_id-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1488397707--com.google.api-client__google-api-client__1.31.3.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--86627061--io.grpc__grpc-services__1.45.1.jar:/databricks/jars/common--principal-context--api--principal_context_proto_library-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1612111443--com.google.http-client__google-http-client__1.38.0.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1861995773--io.opencensus__opencensus-contrib-http-util__0.24.0.jar:/databricks/jars/common--logging--filters--service--service-spark_3.5_2.12_deploy.jar:/databricks/jars/common--database--datasource--standalone-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:/databricks/jars/common--hadoop--filesystem-util-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2---272948794--software.amazon.awssdk__aws-core__2.20.121.jar:/databricks/jars/common--http--service--service-spark_3.5_2.12_deploy.jar:/databricks/jars/common--rpc--rate-limiter--rate-limiter-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--sql--catalyst--libspark-sql-parser-compiled.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded_base---757783023--com.google.errorprone__error_prone_annotations__2.3.4.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.airlift--aircompressor--io.airlift__aircompressor__0.24.jar:/databricks/jars/common--instrumentation--cache-exporter--cache-exporter-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--iceberg-core--core--httpclient5_shaded-for-iceberg-core-shaded-hive-2.3__hadoop-3.2--56328135--org.apache.httpcomponents.client5__httpclient5__5.1.3.jar:/databricks/jars/third_party--tink--tink-shaded_2.12---1389521923--io.netty__netty-codec-http2__4.1.86.Final.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.twitter--util-registry_2.12--com.twitter__util-registry_2.12__7.1.0.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1991970652--io.grpc__grpc-grpclb__1.50.2.jar:/databricks/jars/common--encryption--cpk-deps-shaded--2094972470--com.fasterxml.jackson.core__jackson-databind__2.12.7.1.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--218144780--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/proto--logs--activity--sqlgateway--sqlgateway_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/common--conf--common--rpc-port-conf-spark_3.5_2.12_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-handler--io.netty__netty-handler__4.1.93.Final.jar:/databricks/jars/----ws_3_5--vendor--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2---624154049--software.amazon.awssdk__endpoints-spi__2.20.121.jar:/databricks/jars/----ws_3_5--vendor--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--1620236755--libdelta_connect_proto_lib_unshaded.jar:/databricks/jars/proto--logs--activity--central--central_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/common--database--result-set-mappers--result-set-mappers-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded---2040118540--io.netty__netty-transport-native-unix-common__4.1.86.Final.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1466452907--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/common--logging--tags--encryption--encryption-spark_3.5_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded---577661061--io.projectreactor.netty__reactor-netty-http__1.0.9.jar:/databricks/jars/common--conf--constants--constants-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--protobuf--okio-jvm_only_shaded-for-protobuf-hive-2.3__hadoop-3.2--1333044618--com.squareup.okio__okio-jvm__3.0.0.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--guava-31.1-jre.jar:/databricks/jars/third_party--armeria--armeria_shaded--1052891939--io.netty__netty-transport-classes-epoll__4.1.86.Final.jar:/databricks/jars/third_party--tink--tink-shaded_2.12---850184846--commons-codec__commons-codec__1.11.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.prometheus--simpleclient_servlet--io.prometheus__simpleclient_servlet__0.7.0.jar:/databricks/jars/----ws_3_5--vendor--snowflake--libsnowflake_resources.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.hive--hive-llap-common--org.apache.hive__hive-llap-common__2.3.9.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--103274271--io.netty__netty-codec-http__4.1.86.Final.jar:/databricks/jars/secret-manager--api--api-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--vendor--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12---150294805--io.netty__netty-handler-proxy__4.1.86.Final.jar:/databricks/jars/common--rpc--hooks--require-user-context--require-user-context-spark_3.5_2.12_deploy.jar:/databricks/jars/common--logging--structured--proto-logger-configuration-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---2116158929--io.grpc__grpc-core__1.37.0.jar:/databricks/jars/common--sisyphus--client--client-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--56548403--com.google.apis__google-api-services-iamcredentials__v1-rev20211203-2.0.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-codec-socks--io.netty__netty-codec-socks__4.1.93.Final.jar:/databricks/jars/----ws_3_5--vendor--conduit--netsuite--spark--libnetsuite_resources.jar:/databricks/jars/common--hadoop--s3--filesystem-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1751141346--io.netty__netty-transport__4.1.94.Final.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--7948559--com.fasterxml.jackson.core__jackson-core__2.13.5.jar:/databricks/jars/----ws_3_5--vendor--iceberg--iceberg_clean_shaded-for-iceberg-converter---542754921--software.amazon.awssdk__annotations__2.17.285.jar:/databricks/jars/third_party--armeria--armeria_shaded--1463306973--com.google.errorprone__error_prone_annotations__2.10.0.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1814366213--com.fasterxml.woodstox__woodstox-core__6.4.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common--1860799066--com.google.flogger__flogger-system-backend__0.5.1.jar:/databricks/jars/common--build-info--build-info-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.eclipse.jetty--jetty-jndi--org.eclipse.jetty__jetty-jndi__9.4.52.v20230823.jar:/databricks/jars/third_party--gcs-private--gcs-shaded-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded---1944309733--org.latencyutils__LatencyUtils__2.0.3.jar:/databricks/jars/third_party--armeria--armeria_shaded--1870423252--io.netty__netty-handler__4.1.86.Final.jar:/databricks/jars/----ws_3_5--third_party--kinesis-sdk--kinesis-sdk--512893012--software.amazon.glue__schema-registry-serde__1.0.0.jar:/databricks/jars/common--util--simple-path-spark_3.5_2.12_deploy.jar:/databricks/jars/common--attribution-context--attribution-context-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--grpc-protobuf-lite-1.54.0.jar:/databricks/jars/common--dbsql-config--dbsql-config-spark_3.5_2.12_deploy.jar:/databricks/jars/common--instrumentation--info-service-conf--info-service-conf-spark_3.5_2.12_deploy.jar:/databricks/jars/feature-flag--client--feature_flag_context_utils-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-sns--com.amazonaws__aws-java-sdk-sns__1.12.390.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1573999303--com.google.errorprone__error_prone_annotations__2.10.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.11.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1258094998--javax.inject__javax.inject__1.jar:/databricks/jars/----ws_3_5--patched-hive-with-glue--hive-exec_filtered---1223387842--org.apache.hive__hive-exec__2.3.9-core.jar:/databricks/jars/common--logging--filters--filters-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--1657273854--com.google.j2objc__j2objc-annotations__1.3.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1579068713--com.google.auto.value__auto-value-annotations__1.10.1.jar:/databricks/jars/third_party--gcp-java--gcp-java_shaded---1499494893--com.google.apis__google-api-services-container__v1-rev20221017-2.0.0.jar:/databricks/jars/proto--logs--activity--webapp--webapp_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/spark--driver--events-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.datasketches--datasketches-java--org.apache.datasketches__datasketches-java__3.1.0.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1428008859--com.fasterxml.jackson.core__jackson-annotations__2.14.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----ws_3_5--vendor--connect--server--spark_connect_proto_lib_shaded---744856947--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--commons-io--commons-io--commons-io__commons-io__2.13.0.jar:/databricks/jars/third_party--gcs-private--gcs-shaded_common---1672939599--commons-logging__commons-logging__1.2.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12---773384666--com.nimbusds__nimbus-jose-jwt__9.31.jar:/databricks/jars/third_party--armeria--armeria_shaded--2018211991--com.fasterxml.jackson.core__jackson-core__2.13.5.jar:/databricks/jars/common--logging--log-level-control--log-level-control-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-elasticache--com.amazonaws__aws-java-sdk-elasticache__1.12.390.jar:/databricks/jars/common--jobs--run_id-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1899307381--io.netty__netty-transport-native-unix-common__4.1.86.Final-linux-x86_64.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.postgresql--postgresql--org.postgresql__postgresql__42.6.0.jar:/databricks/jars/proto--logs--sla--service_extra_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1163445989--io.netty__netty-codec-socks__4.1.86.Final.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1725025073--io.netty__netty-codec-socks__4.1.94.Final.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--google-auth-library-credentials-1.16.0.jar:/databricks/jars/common--error-code--error-code-utils-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--jets3t-0.7--com.databricks--jets3t--com.databricks__jets3t__0.7.1-0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.eclipse.jetty--jetty-proxy--org.eclipse.jetty__jetty-proxy__9.4.52.v20230823.jar:/databricks/jars/----ws_3_5--vendor--sql-aws-connectors--sql-aws-connectors-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--logging--tags--hadoop--hadoop-spark_3.5_2.12_deploy.jar:/databricks/jars/common--network--network-spark_3.5_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded--2072392509--com.microsoft.azure__azure-storage__8.6.6.jar:/databricks/jars/extern--libaws-regions.jar:/databricks/jars/----ws_3_5--vendor--iceberg--iceberg_c\n\n*** WARNING: max output size exceeded, skipping output. ***\n\n-3.5.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/kafka-tools-api-3.5.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/kafka_2.12-3.5.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/lz4-java-1.8.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/maven-artifact-3.8.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/metrics-core-2.2.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/metrics-core-4.1.12.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-buffer-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-codec-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-common-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-handler-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-resolver-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-classes-epoll-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-native-epoll-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/netty-transport-native-unix-common-4.1.94.Final.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/osgi-resource-locator-1.0.3.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/paranamer-2.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/plexus-utils-3.3.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/reflections-0.9.12.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/reload4j-1.2.25.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/rocksdbjni-7.1.2.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-collection-compat_2.12-2.10.0.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-java8-compat_2.12-1.0.2.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-library-2.12.15.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-logging_2.12-3.9.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/scala-reflect-2.12.15.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/slf4j-api-1.7.36.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/slf4j-reload4j-1.7.36.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/snappy-java-1.1.10.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/swagger-annotations-2.2.8.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/trogdor-3.5.1.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zookeeper-3.6.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zookeeper-jute-3.6.4.jar:/databricks/driver/kafka_2.12-3.5.1/bin/../libs/zstd-jni-1.5.5-1.jar (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,356] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,356] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,356] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,357] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,357] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,357] INFO Client environment:os.version=5.15.0-1043-aws (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:user.name=root (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:user.home=/root (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:user.dir=/databricks/driver (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:os.memory.free=1007MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:os.memory.max=1024MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,358] INFO Client environment:os.memory.total=1024MB (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,402] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@213deac2 (org.apache.zookeeper.ZooKeeper)\n[2023-10-04 21:03:03,493] INFO jute.maxbuffer value is 4194304 Bytes (org.apache.zookeeper.ClientCnxnSocket)\n[2023-10-04 21:03:03,554] INFO zookeeper.request.timeout value is 0. feature enabled=false (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 21:03:03,577] INFO [ZooKeeperClient Kafka server] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 21:03:03,591] INFO Opening socket connection to server localhost/127.0.0.1:2181. (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 21:03:03,610] INFO Socket connection established, initiating session, client: /127.0.0.1:47000, server: localhost/127.0.0.1:2181 (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 21:03:03,655] INFO Session establishment complete on server localhost/127.0.0.1:2181, session id = 0x10002175e770002, negotiated timeout = 18000 (org.apache.zookeeper.ClientCnxn)\n[2023-10-04 21:03:03,700] INFO [ZooKeeperClient Kafka server] Connected. (kafka.zookeeper.ZooKeeperClient)\n[2023-10-04 21:03:04,728] INFO Cluster ID = WZA3jKCPRX2exesgY__ZXw (kafka.server.KafkaServer)\n[2023-10-04 21:03:04,949] INFO KafkaConfig values: \n\tadvertised.listeners = null\n\talter.config.policy.class.name = null\n\talter.log.dirs.replication.quota.window.num = 11\n\talter.log.dirs.replication.quota.window.size.seconds = 1\n\tauthorizer.class.name = \n\tauto.create.topics.enable = true\n\tauto.include.jmx.reporter = true\n\tauto.leader.rebalance.enable = true\n\tbackground.threads = 10\n\tbroker.heartbeat.interval.ms = 2000\n\tbroker.id = 0\n\tbroker.id.generation.enable = true\n\tbroker.rack = null\n\tbroker.session.timeout.ms = 9000\n\tclient.quota.callback.class = null\n\tcompression.type = producer\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = []\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.5-IV2\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL\n\tlisteners = PLAINTEXT://:9092\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dirs = /tmp/kafka-logs\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.manager.class.name = null\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = null\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = null\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 1\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = localhost:2181\n\tzookeeper.connection.timeout.ms = 18000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2023-10-04 21:03:05,255] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 21:03:05,263] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 21:03:05,275] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 21:03:05,310] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2023-10-04 21:03:05,594] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n[2023-10-04 21:03:05,657] INFO Skipping recovery of 1 logs from /tmp/kafka-logs since clean shutdown file was found (kafka.log.LogManager)\n[2023-10-04 21:03:05,968] INFO [LogLoader partition=topic_news-0, dir=/tmp/kafka-logs] Loading producer state till offset 300 with message format version 2 (kafka.log.UnifiedLog$)\n[2023-10-04 21:03:05,970] INFO [LogLoader partition=topic_news-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 300 (kafka.log.UnifiedLog$)\n[2023-10-04 21:03:05,977] INFO [ProducerStateManager partition=topic_news-0]Loading producer state from snapshot file 'SnapshotFile(offset=300, file=/tmp/kafka-logs/topic_news-0/00000000000000000300.snapshot)' (org.apache.kafka.storage.internals.log.ProducerStateManager)\n[2023-10-04 21:03:06,002] INFO [LogLoader partition=topic_news-0, dir=/tmp/kafka-logs] Producer state recovery took 31ms for snapshot load and 0ms for segment recovery from offset 300 (kafka.log.UnifiedLog$)\n[2023-10-04 21:03:06,128] INFO Completed load of Log(dir=/tmp/kafka-logs/topic_news-0, topicId=NCFyoj01Tfuy7FWWv2blMw, topic=topic_news, partition=0, highWatermark=0, lastStableOffset=0, logStartOffset=0, logEndOffset=300) with 1 segments in 451ms (1/1 completed in /tmp/kafka-logs) (kafka.log.LogManager)\n[2023-10-04 21:03:06,137] INFO Loaded 1 logs in 541ms (kafka.log.LogManager)\n[2023-10-04 21:03:06,156] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2023-10-04 21:03:06,158] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2023-10-04 21:03:06,346] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2023-10-04 21:03:06,418] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n[2023-10-04 21:03:06,477] INFO [MetadataCache brokerId=0] Updated cache from existing <empty> to latest FinalizedFeaturesAndEpoch(features=Map(), epoch=0). (kafka.server.metadata.ZkMetadataCache)\n[2023-10-04 21:03:06,663] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 21:03:08,097] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2023-10-04 21:03:08,152] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2023-10-04 21:03:08,170] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 21:03:08,250] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:08,254] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:08,258] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:08,260] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:08,348] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2023-10-04 21:03:08,652] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n[2023-10-04 21:03:08,729] INFO Stat of the created znode at /brokers/ids/0 is: 71,71,1696453388709,1696453388709,1,0,0,72059893430288386,254,0,71\n (kafka.zk.KafkaZkClient)\n[2023-10-04 21:03:08,736] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://1004-194529-xqyu0ihu-10-172-209-186:9092, czxid (broker epoch): 71 (kafka.zk.KafkaZkClient)\n[2023-10-04 21:03:09,068] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:09,112] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:09,120] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:09,257] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2023-10-04 21:03:09,285] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2023-10-04 21:03:09,384] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2023-10-04 21:03:09,391] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2023-10-04 21:03:09,399] INFO [TxnMarkerSenderThread-0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2023-10-04 21:03:09,685] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:09,700] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-194529-xqyu0ihu-10-172-209-186/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:09,807] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:09,851] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2023-10-04 21:03:09,912] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:09,912] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-194529-xqyu0ihu-10-172-209-186/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:09,914] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:10,006] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n[2023-10-04 21:03:10,017] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:10,017] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1004-194529-xqyu0ihu-10-172-209-186/127.0.1.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:10,018] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2023-10-04 21:03:10,047] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n[2023-10-04 21:03:10,055] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2023-10-04 21:03:10,126] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 21:03:10,127] INFO Kafka commitId: 30969fa33c185e88 (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 21:03:10,128] INFO Kafka startTimeMs: 1696453390110 (org.apache.kafka.common.utils.AppInfoParser)\n[2023-10-04 21:03:10,137] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n[2023-10-04 21:03:10,527] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new controller, from now on will use node 1004-194529-xqyu0ihu-10-172-209-186:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 21:03:10,527] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new controller, from now on will use node 1004-194529-xqyu0ihu-10-172-209-186:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)\n[2023-10-04 21:03:10,752] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(topic_news-0) (kafka.server.ReplicaFetcherManager)\n[2023-10-04 21:03:10,784] INFO [Partition topic_news-0 broker=0] Log loaded for partition topic_news-0 with initial high watermark 300 (kafka.cluster.Partition)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh \n",
    "./kafka_2.12-3.5.1/bin/kafka-server-start.sh ./kafka_2.12-3.5.1/config/server.properties\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2807722162719343,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Kafka_Management",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
